{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pathology Analysis Tutorial\n",
    "\n",
    "This notebook demonstrates how to use the `onem_path` module for dual-mode pathology image analysis: traditional CellProfiler-based radiomics and deep transfer learning with TITAN model.\n",
    "\n",
    "## üìã Table of Contents\n",
    "1. [Setup and Imports](#setup)\n",
    "2. [CellProfiler Feature Extraction](#cellprofiler)\n",
    "3. [TITAN Deep Learning Features](#titan)\n",
    "4. [Combined Feature Analysis](#combined)\n",
    "5. [WSI Processing](#wsi)\n",
    "6. [Feature Fusion and Selection](#fusion)\n",
    "7. [Comparative Analysis](#comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup and Imports {#setup}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Image processing imports\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import openslide\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import onem_path modules\n",
    "from onem_path import PathologyAnalyzer\n",
    "from onem_path.extractors.cellprofiler_extractor import CellProfilerExtractor\n",
    "from onem_path.extractors.titan_extractor import TITANExtractor\n",
    "from onem_path.config.settings import get_preset_config\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All modules imported successfully!\")\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ CellProfiler Feature Extraction {#cellprofiler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pathology analyzer\n",
    "analyzer = PathologyAnalyzer()\n",
    "print(\"üß™ Pathology analyzer initialized\")\n",
    "\n",
    "# Setup paths\n",
    "image_dir = \"sample_data/pathology_images/\"\n",
    "output_dir = \"output/pathology_features/\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Check if sample data exists\n",
    "if os.path.exists(image_dir):\n",
    "    print(f\"üìÅ Processing pathology images from: {image_dir}\")\n",
    "    \n",
    "    # Extract CellProfiler features\n",
    "    print(\"üöÄ Extracting CellProfiler features...\")\n",
    "    \n",
    "    cp_features = analyzer.extract_features(\n",
    "        image_dir=image_dir,\n",
    "        method='cellprofiler',\n",
    "        config_name='default',\n",
    "        output_csv=os.path.join(output_dir, 'cellprofiler_features.csv'),\n",
    "        parallel=True,\n",
    "        n_workers=4\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ CellProfiler extraction completed!\")\n",
    "    print(f\"üìä Features saved to: {output_dir}/cellprofiler_features.csv\")\n",
    "    \n",
    "    if cp_features:\n",
    "        # Convert to DataFrame for analysis\n",
    "        cp_df = pd.DataFrame(cp_features).T  # Transpose to have features as columns\n",
    "        cp_df.index.name = 'ImageID'\n",
    "        cp_df.reset_index(inplace=True)\n",
    "        \n",
    "        print(f\"üìà Processed {len(cp_df)} images\")\n",
    "        print(f\"üìã Extracted {len(cp_df.columns) - 1} feature types\")  # -1 for ImageID\n",
    "        \n",
    "        # Display feature categories\n",
    "        feature_categories = {\n",
    "            'Nuclear Features': [col for col in cp_df.columns if 'nuclear' in col.lower()],\n",
    "            'Cellular Features': [col for col in cp_df.columns if 'cell' in col.lower()],\n",
    "            'Texture Features': [col for col in cp_df.columns if any(tex in col.lower() \n",
    "                               for tex in ['texture', 'haralick', 'glcm'])],\n",
    "            'Morphological': [col for col in cp_df.columns if any(morph in col.lower() \n",
    "                              for morph in ['area', 'perimeter', 'shape', 'circularity'])]\n",
    "        }\n",
    "        \n",
    "        print(\"\\nüîç Feature Categories:\")\n",
    "        for category, features in feature_categories.items():\n",
    "            print(f\"  {category}: {len(features)} features\")\n",
    "            if features:\n",
    "                print(f\"    Sample: {features[:3]}...\")\nelse:\n",
    "    print(f\"‚ö†Ô∏è  Sample directory not found: {image_dir}\")\n",
    "    print(\"Please replace with your actual pathology image directory\")\n",
    "    \n",
    "# Create dummy CellProfiler data for demonstration\n",
    "print(\"\\nüé≠ Creating dummy CellProfiler features for demonstration...\")\n",
    "dummy_cp_data = {\n",
    "    'Image_001': {\n",
    "        'nuclear_area_mean': 45.2, 'nuclear_perimeter_mean': 23.8, 'nuclear_circularity_mean': 0.78,\n",
    "        'cell_area_mean': 89.5, 'cell_perimeter_mean': 34.2, 'cell_eccentricity_mean': 0.65,\n",
    "        'texture_glcm_contrast': 0.23, 'texture_glcm_homogeneity': 0.87, 'texture_glcm_entropy': 1.45,\n",
    "        'morphological_solidity': 0.92, 'morphological_extent': 0.68, 'morphological_aspect_ratio': 1.23\n",
    "    },\n",
    "    'Image_002': {\n",
    "        'nuclear_area_mean': 52.8, 'nuclear_perimeter_mean': 25.4, 'nuclear_circularity_mean': 0.81,\n",
    "        'cell_area_mean': 95.3, 'cell_perimeter_mean': 36.8, 'cell_eccentricity_mean': 0.59,\n",
    "        'texture_glcm_contrast': 0.31, 'texture_glcm_homogeneity': 0.82, 'texture_glcm_entropy': 1.67,\n",
    "        'morphological_solidity': 0.89, 'morphological_extent': 0.71, 'morphological_aspect_ratio': 1.18\n",
    "    },\n",
    "    'Image_003': {\n",
    "        'nuclear_area_mean': 38.9, 'nuclear_perimeter_mean': 21.2, 'nuclear_circularity_mean': 0.74,\n",
    "        'cell_area_mean': 78.6, 'cell_perimeter_mean': 30.5, 'cell_eccentricity_mean': 0.71,\n",
    "        'texture_glcm_contrast': 0.18, 'texture_glcm_homogeneity': 0.91, 'texture_glcm_entropy': 1.23,\n",
    "        'morphological_solidity': 0.94, 'morphological_extent': 0.65, 'morphological_aspect_ratio': 1.35\n",
    "    }\n",
    "}\n",
    "\n",
    "cp_df = pd.DataFrame.from_dict(dummy_cp_data, orient='index')\n",
    "cp_df.index.name = 'ImageID'\n",
    "cp_df.reset_index(inplace=True)\n",
    "\n",
    "print(f\"üìä Dummy CellProfiler features created: {len(cp_df)} images, {len(cp_df.columns) - 1} features\")\n",
    "print(\"\\nüëÄ Sample CellProfiler features:\")\n",
    "display(cp_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ TITAN Deep Learning Features {#titan}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract TITAN deep learning features\n",
    "if os.path.exists(image_dir):\n",
    "    print(\"üöÄ Extracting TITAN deep learning features...\")\n",
    "    \n",
    "    titan_features = analyzer.extract_features(\n",
    "        image_dir=image_dir,\n",
    "        method='titan',\n",
    "        config_name='titan_pretrained',\n",
    "        output_csv=os.path.join(output_dir, 'titan_features.csv'),\n",
    "        model_type='resnet50',  # or 'efficientnet', 'vit', etc.\n",
    "        feature_layer='last_conv',  # Extract from last convolutional layer\n",
    "        parallel=True,\n",
    "        n_workers=2  # TITAN uses more memory, so fewer workers\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ TITAN extraction completed!\")\n",
    "    print(f\"üìä Features saved to: {output_dir}/titan_features.csv\")\n",
    "    \n",
    "    if titan_features:\n",
    "        # Convert to DataFrame\n",
    "        titan_df = pd.DataFrame(titan_features).T\n",
    "        titan_df.index.name = 'ImageID'\n",
    "        titan_df.reset_index(inplace=True)\n",
    "        \n",
    "        print(f\"üìà Processed {len(titan_df)} images\")\n",
    "        print(f\"üìã Extracted {len(titan_df.columns) - 1} deep features\")\n",
    "        \n",
    "        # Analyze deep feature dimensions\n",
    "        feature_cols = [col for col in titan_df.columns if col != 'ImageID']\n",
    "        print(f\"\\nüîç Deep Feature Analysis:\")\n",
    "        print(f\"  Feature dimension: {len(feature_cols)}\")\n",
    "        print(f\"  Feature range: [{titan_df[feature_cols].min().min():.4f}, {titan_df[feature_cols].max().max():.4f}]\")\n",
    "        print(f\"  Feature mean: {titan_df[feature_cols].mean().mean():.4f}\")\n",
    "        print(f\"  Feature std: {titan_df[feature_cols].std().mean():.4f}\")\nelse:\n",
    "    print(f\"‚ö†Ô∏è  Sample directory not found: {image_dir}\")\n",
    "    \n",
    "# Create dummy TITAN features for demonstration\n",
    "print(\"\\nüé≠ Creating dummy TITAN features for demonstration...\")\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "dummy_titan_data = {}\n",
    "feature_dim = 2048  # Typical ResNet50 feature dimension\n",
    "\n",
    "for i, image_id in enumerate(cp_df['ImageID']):\n",
    "    # Create realistic deep features with some structure\n",
    "    base_features = np.random.randn(feature_dim) * 0.1\n",
    "    \n",
    "    # Add some image-specific patterns\n",
    "    if i == 0:  # Image_001\n",
    "        base_features[:100] += 0.3\n",
    "        base_features[100:200] -= 0.2\n",
    "    elif i == 1:  # Image_002\n",
    "        base_features[:100] -= 0.2\n",
    "        base_features[200:300] += 0.4\n",
    "    else:  # Image_003\n",
    "        base_features[300:400] += 0.3\n",
    "        base_features[400:500] -= 0.1\n",
    "    \n",
    "    dummy_titan_data[image_id] = base_features.tolist()\n",
    "\n",
    "titan_df = pd.DataFrame.from_dict(dummy_titan_data, orient='index')\n",
    "titan_df.index.name = 'ImageID'\n",
    "titan_df.reset_index(inplace=True)\n",
    "\n",
    "print(f\"üìä Dummy TITAN features created: {len(titan_df)} images, {len(titan_df.columns) - 1} features\")\n",
    "print(\"\\nüëÄ Sample TITAN features (first 10 dimensions):\")\n",
    "display(titan_df[['ImageID'] + [col for col in titan_df.columns if col != 'ImageID'][:10]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Combined Feature Analysis {#combined}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine CellProfiler and TITAN features\n",
    "if 'cp_df' in locals() and 'titan_df' in locals():\n",
    "    print(\"üîó Combining CellProfiler and TITAN features...\")\n",
    "    \n",
    "    # Merge on ImageID\n",
    "    combined_df = pd.merge(cp_df, titan_df, on='ImageID', suffixes=('_cp', '_titan'))\n",
    "    \n",
    "    print(f\"‚úÖ Features combined successfully!\")\n",
    "    print(f\"üìä Combined dataset: {len(combined_df)} images, {len(combined_df.columns) - 1} total features\")\n",
    "    \n",
    "    # Separate feature types\n",
    "    cp_features = [col for col in combined_df.columns if col.endswith('_cp') or col == 'ImageID']\n",
    "    titan_features = [col for col in combined_df.columns if col.endswith('_titan')]\n",
    "    \n",
    "    print(f\"\\nüîç Feature Breakdown:\")\n",
    "    print(f\"  CellProfiler features: {len(cp_features) - 1}\")  # -1 for ImageID\n",
    "    print(f\"  TITAN features: {len(titan_features)}\")\n",
    "    print(f\"  Total features: {len(combined_df.columns) - 1}\")\n",
    "    \n",
    "    # Feature statistics comparison\n",
    "    print(f\"\\nüìà Feature Statistics Comparison:\")\n",
    "    \n",
    "    # CellProfiler stats\n",
    "    cp_cols = [col for col in cp_features if col != 'ImageID']\n",
    "    if cp_cols:\n",
    "        cp_stats = {\n",
    "            'Mean': combined_df[cp_cols].mean().mean(),\n",
    "            'Std': combined_df[cp_cols].std().mean(),\n",
    "            'Min': combined_df[cp_cols].min().min(),\n",
    "            'Max': combined_df[cp_cols].max().max()\n",
    "        }\n",
    "        print(f\"\\n  CellProfiler Features:\")\n",
    "        for stat, value in cp_stats.items():\n",
    "            print(f\"    {stat}: {value:.4f}\")\n",
    "    \n",
    "    # TITAN stats\n",
    "    if titan_features:\n",
    "        titan_stats = {\n",
    "            'Mean': combined_df[titan_features].mean().mean(),\n",
    "            'Std': combined_df[titan_features].std().mean(),\n",
    "            'Min': combined_df[titan_features].min().min(),\n",
    "            'Max': combined_df[titan_features].max().max()\n",
    "        }\n",
    "        print(f\"\\n  TITAN Features:\")\n",
    "        for stat, value in titan_stats.items():\n",
    "            print(f\"    {stat}: {value:.4f}\")\n",
    "    \n",
    "    # Correlation between feature types\n",
    "    print(f\"\\nüîó Cross-Feature Type Correlation:\")\n",
    "    \n",
    "    # Compute average correlation between CP and TITAN features\n",
    "    if cp_cols and titan_features:\n",
    "        cross_correlations = []\n",
    "        for cp_col in cp_cols[:5]:  # Sample to avoid computation explosion\n",
    "            for titan_col in titan_features[:5]:\n",
    "                corr = combined_df[cp_col].corr(combined_df[titan_col])\n",
    "                cross_correlations.append(corr)\n",
    "        \n",
    "        avg_cross_corr = np.mean(cross_correlations)\n",
    "        print(f\"  Average CP-TITAN correlation: {avg_cross_corr:.4f}\")\nelse:\n",
    "    print(\"‚ö†Ô∏è  Cannot combine features - missing data\")\n",
    "    \n",
    "# Save combined features\n",
    "if 'combined_df' in locals():\n",
    "    combined_output = os.path.join(output_dir, 'combined_pathology_features.csv')\n",
    "    combined_df.to_csv(combined_output, index=False)\n",
    "    print(f\"\\nüíæ Combined features saved to: {combined_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ WSI Processing {#wsi}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Whole Slide Image processing\n",
    "wsi_path = \"sample_data/pathology_wsi/sample_slide.svs\"\n",
    "wsi_output_dir = \"output/wsi_analysis/\"\n",
    "\n",
    "os.makedirs(wsi_output_dir, exist_ok=True)\n",
    "\n",
    "# Check if WSI file exists\n",
    "if os.path.exists(wsi_path):\n",
    "    print(f\"üî¨ Processing Whole Slide Image: {wsi_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Open the WSI\n",
    "        slide = openslide.OpenSlide(wsi_path)\n",
    "        \n",
    "        # Get slide information\n",
    "        slide_info = {\n",
    "            'dimensions': slide.dimensions,\n",
    "            'level_count': slide.level_count,\n",
    "            'level_downsamples': slide.level_downsamples,\n",
    "            'vendor': slide.properties.get(openslide.PROPERTY_NAME_VENDOR, 'Unknown'),\n",
    "            'magnification': slide.properties.get(openslide.PROPERTY_NAME_OBJECTIVE_POWER, 'Unknown')\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüìä Slide Information:\")\n",
    "        for key, value in slide_info.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        # Extract patches for analysis\n",
    "        patch_size = 512\n",
    "        patch_overlap = 128\n",
    "        analysis_level = 2  # Use a mid-resolution level for analysis\n",
    "        \n",
    "        print(f\"\\nüîç Extracting patches (size: {patch_size}x{patch_size}, overlap: {patch_overlap})\")\n",
    "        \n",
    "        # Calculate patch positions\n",
    "        level_dimensions = slide.level_dimensions[analysis_level]\n",
    "        step_size = patch_size - patch_overlap\n",
    "        \n",
    "        patches = []\n",
    "        for y in range(0, level_dimensions[1] - patch_size + 1, step_size):\n",
    "            for x in range(0, level_dimensions[0] - patch_size + 1, step_size):\n",
    "                # Extract patch\n",
    "                patch = slide.read_region(\n",
    "                    (x * slide.level_downsamples[analysis_level], \n",
    "                     y * slide.level_downsamples[analysis_level]),\n",
    "                    analysis_level,\n",
    "                    (patch_size, patch_size)\n",
    "                )\n",
    "                patches.append({\n",
    "                    'x': x, 'y': y,\n",
    "                    'patch': patch,\n",
    "                    'patch_rgb': patch.convert('RGB')\n",
    "                })\n",
    "        \n",
    "        print(f\"‚úÖ Extracted {len(patches)} patches\")\n",
    "        \n",
    "        # Analyze first few patches with both methods\n",
    "        print(f\"\\nüî¨ Analyzing patches with both methods...\")\n",
    "        \n",
    "        sample_patches = patches[:5]  # Analyze first 5 patches\n",
    "        wsi_results = []\n",
    "        \n",
    "        for i, patch_data in enumerate(sample_patches):\n",
    "            # Save patch temporarily\n",
    "            patch_path = os.path.join(wsi_output_dir, f'patch_{i:03d}.png')\n",
    "            patch_data['patch_rgb'].save(patch_path)\n",
    "            \n",
    "            # Extract features using both methods\n",
    "            cp_features = analyzer.extract_features(\n",
    "                image_dir=wsi_output_dir,\n",
    "                method='cellprofiler',\n",
    "                config_name='nuclear_focused',\n",
    "                file_pattern=f'patch_{i:03d}.png'\n",
    "            )\n",
    "            \n",
    "            titan_features = analyzer.extract_features(\n",
    "                image_dir=wsi_output_dir,\n",
    "                method='titan',\n",
    "                config_name='titan_pretrained',\n",
    "                file_pattern=f'patch_{i:03d}.png'\n",
    "            )\n",
    "            \n",
    "            wsi_results.append({\n",
    "                'patch_id': i,\n",
    "                'x': patch_data['x'],\n",
    "                'y': patch_data['y'],\n",
    "                'cp_features': cp_features,\n",
    "                'titan_features': titan_features\n",
    "            })\n",
    "            \n",
    "            # Clean up temporary patch file\n",
    "            if os.path.exists(patch_path):\n",
    "                os.remove(patch_path)\n",
    "        \n",
    "        print(f\"‚úÖ WSI patch analysis completed for {len(wsi_results)} patches\")\n",
    "        \n",
    "        # Close slide\n",
    "        slide.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing WSI: {e}\")\nelse:\n",
    "    print(f\"‚ö†Ô∏è  WSI file not found: {wsi_path}\")\n",
    "    print(\"This is a demonstration - replace with your actual SVS file path\")\n",
    "    \n",
    "# Create dummy WSI results for demonstration\n",
    "print(\"\\nüé≠ Creating dummy WSI patch analysis for demonstration...\")\n",
    "\n",
    "dummy_wsi_results = [\n",
    "    {'patch_id': 0, 'x': 0, 'y': 0, 'cp_nuclear_count': 45, 'titan_class': 'epithelial'},\n",
    "    {'patch_id': 1, 'x': 384, 'y': 0, 'cp_nuclear_count': 32, 'titan_class': 'stromal'},\n",
    "    {'patch_id': 2, 'x': 768, 'y': 0, 'cp_nuclear_count': 67, 'titan_class': 'tumor'},\n",
    "    {'patch_id': 3, 'x': 0, 'y': 384, 'cp_nuclear_count': 28, 'titan_class': 'stromal'},\n",
    "    {'patch_id': 4, 'x': 384, 'y': 384, 'cp_nuclear_count': 53, 'titan_class': 'tumor'}\n",
    "]\n",
    "\n",
    "wsi_df = pd.DataFrame(dummy_wsi_results)\n",
    "print(f\"üìä Dummy WSI analysis created: {len(wsi_df)} patches\")\n",
    "print(\"\\nüëÄ Sample WSI patch analysis:\")\n",
    "display(wsi_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Feature Fusion and Selection {#fusion}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced feature fusion techniques\n",
    "if 'combined_df' in locals():\n",
    "    print(\"üéØ Performing feature fusion and selection...\")\n",
    "    \n",
    "    from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    # Separate features\n",
    "    feature_cols = [col for col in combined_df.columns if col != 'ImageID']\n",
    "    X = combined_df[feature_cols]\n",
    "    \n",
    "    # 1. Statistical feature selection\n",
    "    print(\"\\nüìä Statistical Feature Selection:\")\n",
    "    \n",
    "    # Remove features with low variance\n",
    "    variance_threshold = 0.01\n",
    "    low_variance_features = X.columns[X.var() < variance_threshold].tolist()\n",
    "    print(f\"  Low variance features (< {variance_threshold}): {len(low_variance_features)}\")\n",
    "    \n",
    "    # Remove highly correlated features\n",
    "    corr_matrix = X.corr().abs()\n",
    "    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    high_corr_features = [col for col in upper_triangle.columns if any(upper_triangle[col] > 0.95)]\n",
    "    print(f\"  Highly correlated features (r > 0.95): {len(high_corr_features)}\")\n",
    "    \n",
    "    # 2. Dimensionality reduction with PCA\n",
    "    print(\"\\nüîç PCA Dimensionality Reduction:\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Determine optimal number of components (95% variance)\n",
    "    pca_full = PCA()\n",
    "    pca_full.fit(X_scaled)\n",
    "    \n",
    "    cumsum_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "    n_components_95 = np.argmax(cumsum_variance >= 0.95) + 1\n",
    "    \n",
    "    print(f\"  Original features: {len(feature_cols)}\")\n",
    "    print(f\"  Components for 95% variance: {n_components_95}\")\n",
    "    print(f\"  Variance reduction: {(1 - n_components_95/len(feature_cols))*100:.1f}%\")\n",
    "    \n",
    "    # Apply PCA with optimal components\n",
    "    pca = PCA(n_components=n_components_95)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Create DataFrame with PCA features\n",
    "    pca_feature_names = [f'PCA_{i+1}' for i in range(n_components_95)]\n",
    "    pca_df = pd.DataFrame(X_pca, columns=pca_feature_names)\n",
    "    pca_df['ImageID'] = combined_df['ImageID']\n",
    "    \n",
    "    # 3. Feature importance analysis (if we had labels)\n",
    "    print(\"\\nüéØ Feature Importance Analysis:\")\n",
    "    print(\"  (Note: Would require class labels for supervised selection)\")\n",
    "    \n",
    "    # Analyze PCA components\n",
    "    print(f\"\\nüìà PCA Component Analysis (first 5 components):\")\n",
    "    for i in range(min(5, n_components_95)):\n",
    "        explained_var = pca.explained_variance_ratio_[i]\n",
    "        print(f\"  PCA_{i+1}: {explained_var:.4f} ({explained_var*100:.2f}% variance)\")\nelse:\n",
    "    print(\"‚ö†Ô∏è  No combined features available for fusion analysis\")\n",
    "    \n",
    "# Save fused features\n",
    "if 'pca_df' in locals():\n",
    "    fused_output = os.path.join(output_dir, 'fused_pathology_features.csv')\n",
    "    pca_df.to_csv(fused_output, index=False)\n",
    "    print(f\"\\nüíæ Fused features saved to: {fused_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Comparative Analysis {#comparison}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparative visualizations\n",
    "if 'cp_df' in locals() and 'titan_df' in locals():\n",
    "    print(\"üìä Creating comparative analysis visualizations...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Pathology Feature Analysis Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Feature count comparison\n",
    "    cp_feature_count = len([col for col in cp_df.columns if col != 'ImageID'])\n",
    "    titan_feature_count = len([col for col in titan_df.columns if col != 'ImageID'])\n",
    "    \n",
    "    feature_counts = [cp_feature_count, titan_feature_count]\n",
    "    feature_labels = ['CellProfiler', 'TITAN']\n",
    "    \n",
    "    axes[0, 0].bar(feature_labels, feature_counts, color=['skyblue', 'lightcoral'], alpha=0.7)\n",
    "    axes[0, 0].set_title('Feature Count Comparison')\n",
    "    axes[0, 0].set_ylabel('Number of Features')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for i, count in enumerate(feature_counts):\n",
    "        axes[0, 0].text(i, count + max(feature_counts)*0.01, str(count), \n",
    "                       ha='center', fontweight='bold')\n",
    "    \n",
    "    # 2. Feature distribution comparison\n",
    "    cp_cols = [col for col in cp_df.columns if col != 'ImageID']\n",
    "    titan_cols = [col for col in titan_df.columns if col != 'ImageID']\n",
    "    \n",
    "    if cp_cols and titan_cols:\n",
    "        # Sample features for visualization\n",
    "        cp_sample = cp_df[cp_cols[:5]].values.flatten()\n",
    "        titan_sample = titan_df[titan_cols[:100]].values.flatten()  # Sample more TITAN features\n",
    "        \n",
    "        axes[0, 1].hist(cp_sample, bins=30, alpha=0.7, label='CellProfiler', \n",
    "                       color='skyblue', density=True)\n",
    "        axes[0, 1].hist(titan_sample, bins=30, alpha=0.7, label='TITAN', \n",
    "                       color='lightcoral', density=True)\n",
    "        axes[0, 1].set_title('Feature Value Distribution')\n",
    "        axes[0, 1].set_xlabel('Feature Value')\n",
    "    axes[0, 1].set_ylabel('Density')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Correlation heatmap (subset)\n",
    "    if 'combined_df' in locals():\n",
    "        # Create correlation matrix with sample features\n",
    "        sample_features = []\n",
    "        sample_features.extend(cp_cols[:3])\n",
    "        sample_features.extend(titan_cols[:7])  # Total 10 features\n",
    "        \n",
    "        if len(sample_features) >= 10:\n",
    "            corr_subset = combined_df[sample_features].corr()\n",
    "            \n",
    "            # Create masks for different feature types\n",
    "            cp_mask = np.zeros_like(corr_subset, dtype=bool)\n",
    "            titan_mask = np.zeros_like(corr_subset, dtype=bool)\n",
    "            \n",
    "            for i, feat1 in enumerate(sample_features):\n",
    "                for j, feat2 in enumerate(sample_features):\n",
    "                    if feat1 in cp_cols and feat2 in cp_cols:\n",
    "                        cp_mask[i, j] = True\n",
    "                    elif feat1 in titan_cols and feat2 in titan_cols:\n",
    "                        titan_mask[i, j] = True\n",
    "            \n",
    "            sns.heatmap(corr_subset, annot=True, cmap='coolwarm', center=0, \n",
    "                       square=True, fmt='.2f', ax=axes[0, 2], cbar_kws={\"shrink\": .6})\n",
    "            axes[0, 2].set_title('Feature Correlation Matrix\\n(First 10 Features)')\n",
    "    \n",
    "    # 4. PCA variance explained\n",
    "    if 'pca' in locals():\n",
    "        n_components_show = min(20, len(pca.explained_variance_ratio_))\n",
    "        component_numbers = range(1, n_components_show + 1)\n",
    "        \n",
    "        axes[1, 0].bar(component_numbers, \n",
    "                       pca.explained_variance_ratio_[:n_components_show],\n",
    "                       color='gold', alpha=0.7)\n",
    "        axes[1, 0].set_title('PCA Explained Variance Ratio\\n(First 20 Components)')\n",
    "        axes[1, 0].set_xlabel('Principal Component')\n",
    "        axes[1, 0].set_ylabel('Explained Variance Ratio')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Cumulative variance\n",
    "    if 'pca' in locals():\n",
    "        cumsum_var = np.cumsum(pca.explained_variance_ratio_[:n_components_show])\n",
    "        \n",
    "        axes[1, 1].plot(component_numbers, cumsum_var, 'o-', \n",
    "                       color='green', linewidth=2, markersize=6)\n",
    "        axes[1, 1].axhline(y=0.95, color='red', linestyle='--', alpha=0.7, label='95% variance')\n",
    "        axes[1, 1].axhline(y=0.90, color='orange', linestyle='--', alpha=0.7, label='90% variance')\n",
    "        axes[1, 1].set_title('Cumulative Explained Variance')\n",
    "        axes[1, 1].set_xlabel('Number of Components')\n",
    "        axes[1, 1].set_ylabel('Cumulative Variance Ratio')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. WSI patch analysis (if available)\n",
    "    if 'wsi_df' in locals():\n",
    "        patch_classes = wsi_df['titan_class'].value_counts()\n",
    "        \n",
    "        axes[1, 2].pie(patch_classes.values, labels=patch_classes.index, \n",
    "                      autopct='%1.1f%%', colors=['lightgreen', 'lightyellow', 'lightpink'])\n",
    "        axes[1, 2].set_title('WSI Patch Classification\\n(TITAN Results)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\nelse:\n",
    "    print(\"‚ö†Ô∏è  Insufficient data for comparative analysis\")\n",
    "    \n",
    "# Create a summary comparison table\n",
    "if 'cp_df' in locals() and 'titan_df' in locals():\n",
    "    comparison_data = {\n",
    "        'Method': ['CellProfiler', 'TITAN', 'Combined'],\n",
    "        'Feature Count': [cp_feature_count, titan_feature_count, cp_feature_count + titan_feature_count],\n",
    "        'Feature Type': ['Traditional Radiomics', 'Deep Learning', 'Hybrid'],\n",
    "        'Interpretability': ['High', 'Low', 'Medium'],\n",
    "        'Computational Cost': ['Low', 'High', 'High'],\n",
    "        'Domain Specificity': ['High', 'Medium', 'High']\n",
    "    }\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    print(\"\\nüìã Method Comparison Summary:\")\n",
    "    display(comparison_df)\nelse:\n",
    "    # Create dummy comparison table\n",
    "    dummy_comparison = pd.DataFrame({\n",
    "        'Method': ['CellProfiler', 'TITAN', 'Combined'],\n",
    "        'Feature Count': [12, 2048, 2060],\n",
    "        'Feature Type': ['Traditional Radiomics', 'Deep Learning', 'Hybrid'],\n",
    "        'Interpretability': ['High', 'Low', 'Medium'],\n",
    "        'Computational Cost': ['Low', 'High', 'High'],\n",
    "        'Domain Specificity': ['High', 'Medium', 'High']\n",
    "    })\n",
    "    \n",
    "    print(\"\\nüìã Method Comparison Summary (Dummy):\")\n",
    "    display(dummy_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Summary and Best Practices\n",
    "\n",
    "### Key Takeaways:\n",
    "1. **Dual-Mode Analysis**: CellProfiler provides interpretable features, TITAN provides powerful deep features\n",
    "2. **Feature Complementarity**: Traditional and deep features capture different aspects of pathology\n",
    "3. **Dimensionality Reduction**: Essential for high-dimensional deep features\n",
    "4. **WSI Processing**: Enables comprehensive tissue-level analysis\n",
    "5. **Feature Fusion**: Combines strengths of both approaches\n",
    "\n",
    "### Method Selection Guidelines:\n",
    "- **CellProfiler**: Best for interpretability, clinical applications, limited data\n",
    "- **TITAN**: Best for large datasets, research, maximum performance\n",
    "- **Combined**: Best for comprehensive analysis, research projects\n",
    "\n",
    "### Performance Considerations:\n",
    "- ‚ö° **CellProfiler**: Fast processing (~1-2 sec per image)\n",
    "- ‚ö° **TITAN**: Slower processing (~5-10 sec per image, GPU recommended)\n",
    "- ‚ö° **Combined**: Combined processing time\n",
    "- üíæ **Memory**: TITAN requires more memory, especially for WSI\n",
    "\n",
    "### Common Issues and Solutions:\n",
    "- ‚ö†Ô∏è **Low image quality** ‚Üí Apply preprocessing and quality control\n",
    "- ‚ö†Ô∏è **Memory issues** ‚Üí Use smaller patches or batch processing\n",
    "- ‚ö†Ô∏è **Feature redundancy** ‚Üí Apply correlation filtering and PCA\n",
    "- ‚ö†Ô∏è **Domain mismatch** ‚Üí Fine-tune models on specific tissue types\n",
    "\n",
    "### Next Steps:\n",
    "- üß™ Validate features with clinical outcomes\n",
    "- üîó Combine with radiology features for multi-modal analysis\n",
    "- üìä Build predictive models using extracted features\n",
    "- üéØ Deploy in clinical workflow with proper validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}