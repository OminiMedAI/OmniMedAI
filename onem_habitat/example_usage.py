"""
onem_habitat ä½¿ç”¨ç¤ºä¾‹
"""

import os
import numpy as np
from pathlib import Path

# å¯¼å…¥ä¸»è¦æ¨¡å—
from onem_habitat.radiomics import LocalRadiomicsExtractor
from onem_habitat.clustering import FeatureClustering
from onem_habitat.segmentation import MaskRefiner
from onem_habitat.config import HabitatConfig, HabitatConfigManager, PRESET_CONFIGS


def example_1_local_features_extraction():
    """ç¤ºä¾‹ 1: å±€éƒ¨æ”¾å°„ç»„å­¦ç‰¹å¾æå–"""
    print("=== ç¤ºä¾‹ 1: å±€éƒ¨æ”¾å°„ç»„å­¦ç‰¹å¾æå– ===")
    
    # åˆ›å»ºç‰¹å¾æå–å™¨
    extractor = LocalRadiomicsExtractor(
        kernel_size=(5, 5, 5),
        feature_types=['firstorder', 'glcm', 'glrlm'],
        bin_width=25,
        n_jobs=1
    )
    
    # æå–å•ä¸ªå›¾åƒçš„ç‰¹å¾
    image_path = "data/images/patient001.nii.gz"
    mask_path = "data/masks/patient001_mask.nii.gz"
    output_path = "output/features/patient001_features.npy"
    
    if os.path.exists(image_path) and os.path.exists(mask_path):
        try:
            result = extractor.extract_local_features(
                image_path=image_path,
                mask_path=mask_path,
                output_path=output_path,
                step_size=2  # æ¯2ä¸ªä½“ç´ æå–ä¸€æ¬¡ï¼ŒåŠ å¿«é€Ÿåº¦
            )
            
            print(f"ç‰¹å¾æå–å®Œæˆ:")
            print(f"  ä½“ç´ æ•°é‡: {result['metadata']['n_voxels']}")
            print(f"  ç‰¹å¾æ•°é‡: {len(result['metadata']['feature_types'])}")
            print(f"  ç‰¹å¾ç±»å‹: {result['metadata']['feature_types']}")
            print(f"  ä¿å­˜åˆ°: {output_path}")
            
            # è·å–ç‰¹å¾æ‘˜è¦
            summary = extractor.get_feature_summary(result)
            print(f"  ç‰¹å¾æ‘˜è¦:")
            for feature_name, stats in list(summary.items())[:3]:  # æ˜¾ç¤ºå‰3ä¸ªç‰¹å¾
                print(f"    {feature_name}: mean={stats['mean']:.3f}, std={stats['std']:.3f}")
                
        except Exception as e:
            print(f"ç‰¹å¾æå–å¤±è´¥: {e}")
    else:
        print(f"æ–‡ä»¶ä¸å­˜åœ¨: {image_path} æˆ– {mask_path}")


def example_2_batch_feature_extraction():
    """ç¤ºä¾‹ 2: æ‰¹é‡ç‰¹å¾æå–"""
    print("\n=== ç¤ºä¾‹ 2: æ‰¹é‡ç‰¹å¾æå– ===")
    
    # åˆ›å»ºç‰¹å¾æå–å™¨
    extractor = LocalRadiomicsExtractor(
        kernel_size=(5, 5, 5),
        feature_types=['firstorder', 'glcm'],
        bin_width=32,
        n_jobs=2
    )
    
    # æ‰¹é‡æå–
    images_dir = "data/images"
    masks_dir = "data/masks"
    output_dir = "output/batch_features"
    
    if os.path.exists(images_dir) and os.path.exists(masks_dir):
        try:
            results = extractor.batch_extract_features(
                images_dir=images_dir,
                masks_dir=masks_dir,
                output_dir=output_dir,
                file_pattern="*.nii.gz",
                step_size=3
            )
            
            print(f"æ‰¹é‡ç‰¹å¾æå–å®Œæˆ:")
            print(f"  å¤„ç†ç»“æœæ•°é‡: {len(results)}")
            
            successful_count = sum(1 for r in results if 'error' not in r)
            print(f"  æˆåŠŸå¤„ç†: {successful_count}")
            print(f"  å¤±è´¥å¤„ç†: {len(results) - successful_count}")
            
            for result in results[:3]:  # æ˜¾ç¤ºå‰3ä¸ªç»“æœ
                if 'error' in result:
                    print(f"  å¤±è´¥: {result['image_file']} - {result['error']}")
                else:
                    print(f"  æˆåŠŸ: {result['image_file']} - {result['n_voxels']} voxels")
                    
        except Exception as e:
            print(f"æ‰¹é‡ç‰¹å¾æå–å¤±è´¥: {e}")
    else:
        print(f"ç›®å½•ä¸å­˜åœ¨: {images_dir} æˆ– {masks_dir}")


def example_3_feature_clustering():
    """ç¤ºä¾‹ 3: ç‰¹å¾èšç±»åˆ†æ"""
    print("\n=== ç¤ºä¾‹ 3: ç‰¹å¾èšç±»åˆ†æ ===")
    
    # åŠ è½½ç‰¹å¾æ•°æ®
    features_files = [
        "output/features/patient001_features.npy",
        "output/features/patient002_features.npy"
    ]
    
    features_dict_list = []
    
    for features_file in features_files:
        if os.path.exists(features_file):
            try:
                features_dict = np.load(features_file, allow_pickle=True).item()
                features_dict_list.append(features_dict)
                print(f"åŠ è½½ç‰¹å¾: {features_file}")
            except Exception as e:
                print(f"åŠ è½½ç‰¹å¾å¤±è´¥ {features_file}: {e}")
    
    if features_dict_list:
        # åˆ›å»ºèšç±»å™¨
        clusterer = FeatureClustering(
            clustering_method='kmeans',
            n_clusters=4,
            feature_selection='variance',
            pca_components=10,
            standardize=True
        )
        
        try:
            # æ‰§è¡Œèšç±»
            cluster_labels_list = clusterer.fit_predict(features_dict_list)
            
            print(f"èšç±»åˆ†æå®Œæˆ:")
            print(f"  å¤„ç†å›¾åƒæ•°é‡: {len(cluster_labels_list)}")
            
            for i, cluster_labels in enumerate(cluster_labels_list):
                unique_labels, counts = np.unique(cluster_labels, return_counts=True)
                print(f"  å›¾åƒ {i+1}: {dict(zip(unique_labels, counts))} ä¸ªèšç±»")
            
            # å¯è§†åŒ–èšç±»ç»“æœ
            if len(cluster_labels_list) > 0 and len(cluster_labels_list[0]) > 0:
                # åˆå¹¶æ‰€æœ‰ç‰¹å¾è¿›è¡Œå¯è§†åŒ–
                all_features, feature_info = clusterer._prepare_features(features_dict_list)
                if all_features is not None:
                    # é‡æ–°æ‰§è¡Œèšç±»ä»¥è·å–æ ‡ç­¾
                    full_labels = clusterer._perform_clustering(
                        clusterer._preprocess_features(
                            clusterer._select_features(all_features, feature_info)
                        )
                    )
                    
                    # ä¿å­˜å¯è§†åŒ–
                    viz_path = "output/clustering_visualization.png"
                    clusterer.visualize_clusters(
                        all_features, full_labels, viz_path, method='tsne'
                    )
                    print(f"  èšç±»å¯è§†åŒ–ä¿å­˜åˆ°: {viz_path}")
            
        except Exception as e:
            print(f"èšç±»åˆ†æå¤±è´¥: {e}")
    else:
        print("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç‰¹å¾æ–‡ä»¶")


def example_4_mask_refinement():
    """ç¤ºä¾‹ 4: åŸºäº Mask é‡æ–°åˆ’åˆ†"""
    print("\n=== ç¤ºä¾‹ 4: åŸºäº Mask é‡æ–°åˆ’åˆ† ===")
    
    # åˆ›å»ºæ©ç ç²¾ç»†å™¨
    refiner = MaskRefiner(
        min_cluster_size=50,
        smoothing_iterations=2,
        connectivity=1,
        fill_holes=True
    )
    
    # åŠ è½½å¿…è¦æ–‡ä»¶
    image_path = "data/images/patient001.nii.gz"
    mask_path = "data/masks/patient001_mask.nii.gz"
    features_path = "output/features/patient001_features.npy"
    labels_path = "output/clustering/clustering_labels_patient001.npy"
    
    if all(os.path.exists(p) for p in [image_path, mask_path, features_path, labels_path]):
        try:
            # åŠ è½½èšç±»æ ‡ç­¾
            cluster_labels = np.load(labels_path)
            
            # åŠ è½½ç‰¹å¾æ•°æ®è·å–åæ ‡
            features_dict = np.load(features_path, allow_pickle=True).item()
            coordinates = features_dict.get('coordinates', [])
            
            # ç¡®ä¿æ ‡ç­¾æ•°é‡åŒ¹é…
            if len(cluster_labels) != len(coordinates):
                print(f"è­¦å‘Š: æ ‡ç­¾æ•°é‡({len(cluster_labels)})ä¸åæ ‡æ•°é‡({len(coordinates)})ä¸åŒ¹é…")
                min_len = min(len(cluster_labels), len(coordinates))
                cluster_labels = cluster_labels[:min_len]
                coordinates = coordinates[:min_len]
            
            output_dir = "output/refined_masks"
            
            # é‡æ–°åˆ’åˆ†æ©ç 
            saved_files = refiner.refine_masks(
                image_path=image_path,
                mask_path=mask_path,
                cluster_labels=cluster_labels,
                coordinates=coordinates,
                output_dir=output_dir,
                save_individual=True,
                save_combined=True
            )
            
            print(f"æ©ç é‡æ–°åˆ’åˆ†å®Œæˆ:")
            print(f"  è¾“å‡ºç›®å½•: {output_dir}")
            print(f"  ä¿å­˜æ–‡ä»¶:")
            for name, path in saved_files.items():
                print(f"    {name}: {path}")
            
            # è¯„ä¼°è´¨é‡
            if 'combined' in saved_files:
                import nibabel as nib
                original_mask_data = nib.load(mask_path).get_fdata()
                refined_masks = {}
                
                # é‡æ–°åŠ è½½ç²¾ç»†åŒ–æ©ç è¿›è¡Œè¯„ä¼°
                for file_name, file_path in saved_files.items():
                    if file_name.startswith('cluster_'):
                        mask_data = nib.load(file_path).get_fdata()
                        refined_masks[file_name] = mask_data
                
                quality_metrics = refiner.evaluate_refinement_quality(
                    original_mask_data, refined_masks, cluster_labels
                )
                
                print(f"  è´¨é‡è¯„ä¼°:")
                for metric_name, value in quality_metrics.items():
                    print(f"    {metric_name}: {value:.3f}")
                    
        except Exception as e:
            print(f"æ©ç é‡æ–°åˆ’åˆ†å¤±è´¥: {e}")
    else:
        print("ç¼ºå°‘å¿…è¦æ–‡ä»¶è¿›è¡Œæ©ç é‡æ–°åˆ’åˆ†")


def example_5_complete_habitat_workflow():
    """ç¤ºä¾‹ 5: å®Œæ•´ç”Ÿæ€åˆ†æå·¥ä½œæµç¨‹"""
    print("\n=== ç¤ºä¾‹ 5: å®Œæ•´ç”Ÿæ€åˆ†æå·¥ä½œæµç¨‹ ===")
    
    # ä½¿ç”¨é¢„è®¾é…ç½®
    config = PRESET_CONFIGS['ct_lung']
    
    # åˆ›å»ºé…ç½®ç®¡ç†å™¨
    config_manager = HabitatConfigManager("config")
    config_manager.save_config(config, name="example_workflow")
    
    print("ä½¿ç”¨é¢„è®¾é…ç½®: CT Lung Habitat Analysis")
    print(f"  æ ¸å¤§å°: {config.kernel_size}")
    print(f"  èšç±»æ•°é‡: {config.n_clusters}")
    print(f"  æœ€å°èšç±»å¤§å°: {config.min_cluster_size}")
    
    # å·¥ä½œæµç¨‹ç›®å½•
    base_dir = "data"
    output_dir = "output/complete_workflow"
    os.makedirs(output_dir, exist_ok=True)
    
    # æ­¥éª¤ 1: ç‰¹å¾æå–
    print("\næ­¥éª¤ 1: ç‰¹å¾æå–...")
    extractor = LocalRadiomicsExtractor(
        kernel_size=config.kernel_size,
        feature_types=config.feature_types,
        bin_width=config.bin_width,
        n_jobs=config.extraction_n_jobs,
        step_size=config.step_size
    )
    
    images_dir = os.path.join(base_dir, "images")
    masks_dir = os.path.join(base_dir, "masks")
    features_dir = os.path.join(output_dir, "features")
    
    extraction_results = []
    if os.path.exists(images_dir) and os.path.exists(masks_dir):
        extraction_results = extractor.batch_extract_features(
            images_dir=images_dir,
            masks_dir=masks_dir,
            output_dir=features_dir,
            step_size=config.step_size
        )
        print(f"  æˆåŠŸæå– {len([r for r in extraction_results if 'error' not in r])} ä¸ªå›¾åƒçš„ç‰¹å¾")
    
    # æ­¥éª¤ 2: ç‰¹å¾èšç±»
    print("\næ­¥éª¤ 2: ç‰¹å¾èšç±»...")
    
    # åŠ è½½æ‰€æœ‰ç‰¹å¾
    features_dict_list = []
    for result in extraction_results:
        if 'error' not in result:
            try:
                features_dict = np.load(result['output_file'], allow_pickle=True).item()
                features_dict_list.append(features_dict)
            except Exception as e:
                print(f"  è·³è¿‡ç‰¹å¾æ–‡ä»¶ {result['output_file']}: {e}")
    
    if features_dict_list:
        clusterer = FeatureClustering(
            clustering_method=config.clustering_method,
            n_clusters=config.n_clusters,
            feature_selection=config.feature_selection,
            pca_components=config.pca_components,
            standardize=config.standardize
        )
        
        cluster_labels_list = clusterer.fit_predict(features_dict_list)
        
        # ä¿å­˜èšç±»ç»“æœ
        clustering_dir = os.path.join(output_dir, "clustering")
        clustering_files = clusterer.save_clustering_results(
            features_dict_list, cluster_labels_list, clustering_dir
        )
        
        print(f"  èšç±»åˆ†æå®Œæˆï¼Œç»“æœä¿å­˜åˆ° {clustering_dir}")
        
        # æ­¥éª¤ 3: æ©ç ç²¾ç»†åˆ’åˆ†
        print("\næ­¥éª¤ 3: æ©ç ç²¾ç»†åˆ’åˆ†...")
        refiner = MaskRefiner(
            min_cluster_size=config.min_cluster_size,
            smoothing_iterations=config.smoothing_iterations,
            connectivity=config.connectivity,
            fill_holes=config.fill_holes
        )
        
        refinement_results = refiner.batch_refine_masks(
            images_dir=images_dir,
            masks_dir=masks_dir,
            features_dir=features_dir,
            clustering_results_dir=clustering_dir,
            output_dir=os.path.join(output_dir, "refined_masks")
        )
        
        successful_refinements = [r for r in refinement_results if 'error' not in r]
        print(f"  æˆåŠŸç²¾ç»†åŒ– {len(successful_refinements)} ä¸ªæ©ç ")
        
        # æ­¥éª¤ 4: ç”ŸæˆæŠ¥å‘Š
        print("\næ­¥éª¤ 4: ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        report = {
            'configuration': config.to_dict(),
            'extraction_summary': {
                'total_images': len(extraction_results),
                'successful_extractions': len(extraction_results) - sum(1 for r in extraction_results if 'error' in r),
                'extraction_results': extraction_results
            },
            'clustering_summary': {
                'total_images_processed': len(cluster_labels_list),
                'average_clusters_per_image': np.mean([len(np.unique(labels)) for labels in cluster_labels_list]) if cluster_labels_list else 0
            },
            'refinement_summary': {
                'total_masks_processed': len(refinement_results),
                'successful_refinements': len(successful_refinements),
                'refinement_results': refinement_results
            }
        }
        
        from onem_habitat.utils import habitat_utils
        report_path = os.path.join(output_dir, "habitat_analysis_report.json")
        habitat_utils.save_json(report, report_path)
        
        print(f"  åˆ†ææŠ¥å‘Šä¿å­˜åˆ°: {report_path}")
        
        print("\nâœ… å®Œæ•´å·¥ä½œæµç¨‹æ‰§è¡Œå®Œæˆ!")
        print(f"  ç»“æœä¿å­˜åœ¨: {output_dir}")
        print(f"  é…ç½®ä¿å­˜åœ¨: config/habitat_config.json")
        
    else:
        print("  æ²¡æœ‰æœ‰æ•ˆçš„ç‰¹å¾æ•°æ®è¿›è¡Œèšç±»")


def example_6_config_management():
    """ç¤ºä¾‹ 6: é…ç½®ç®¡ç†"""
    print("\n=== ç¤ºä¾‹ 6: é…ç½®ç®¡ç† ===")
    
    # åˆ›å»ºé…ç½®ç®¡ç†å™¨
    manager = HabitatConfigManager("config")
    
    # è·å–é»˜è®¤é…ç½®
    default_config = manager.get_default_config()
    print(f"é»˜è®¤é…ç½®:")
    print(f"  æ ¸å¤§å°: {default_config.kernel_size}")
    print(f"  èšç±»æ–¹æ³•: {default_config.clustering_method}")
    print(f"  èšç±»æ•°é‡: {default_config.n_clusters}")
    
    # åˆ›å»ºè‡ªå®šä¹‰é…ç½®
    custom_config = HabitatConfig(
        kernel_size=(7, 7, 7),
        clustering_method='hierarchical',
        n_clusters=5,
        feature_types=['firstorder', 'glcm', 'shape'],
        min_cluster_size=100,
        extraction_n_jobs=4
    )
    
    # éªŒè¯é…ç½®
    validation = custom_config.validate()
    if validation['valid']:
        print("âœ… é…ç½®éªŒè¯é€šè¿‡")
        # ä¿å­˜è‡ªå®šä¹‰é…ç½®
        saved_path = manager.save_config(custom_config, name="custom_example")
        print(f"  é…ç½®ä¿å­˜åˆ°: {saved_path}")
    else:
        print("âŒ é…ç½®éªŒè¯å¤±è´¥:")
        for error in validation['errors']:
            print(f"  é”™è¯¯: {error}")
    
    # åˆ—å‡ºæ‰€æœ‰é¢„è®¾
    presets = manager.list_presets()
    print(f"\nå¯ç”¨é¢„è®¾é…ç½®: {presets}")
    
    # åŠ è½½é¢„è®¾é…ç½®
    if presets:
        preset_name = presets[0]
        preset_config = manager.load_config(preset_name)
        print(f"åŠ è½½é¢„è®¾ '{preset_name}':")
        print(f"  èšç±»æ•°é‡: {preset_config.n_clusters}")
        print(f"  ç‰¹å¾ç±»å‹: {preset_config.feature_types}")


def main():
    """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("onem_habitat ä½¿ç”¨ç¤ºä¾‹\n")
    
    # åˆ›å»ºå¿…è¦çš„è¾“å‡ºç›®å½•
    os.makedirs("output", exist_ok=True)
    os.makedirs("output/features", exist_ok=True)
    os.makedirs("config", exist_ok=True)
    
    # è¿è¡Œç¤ºä¾‹ï¼ˆæ³¨é‡Šæ‰éœ€è¦å®é™…æ•°æ®çš„ç¤ºä¾‹ï¼‰
    example_1_local_features_extraction()
    # example_2_batch_feature_extraction()
    # example_3_feature_clustering()
    # example_4_mask_refinement()
    # example_5_complete_habitat_workflow()
    example_6_config_management()
    
    print("\nç¤ºä¾‹è¿è¡Œå®Œæˆ!")
    print("\nğŸ“ æ³¨æ„:")
    print("- éœ€è¦å®‰è£…ä¾èµ–: pip install pyradiomics scikit-learn matplotlib seaborn scikit-image nibabel")
    print("- å‡†å¤‡æ•°æ®ç›®å½•: data/images/ å’Œ data/masks/")
    print("- å–æ¶ˆæ³¨é‡Šä»¥è¿è¡Œéœ€è¦å®é™…æ•°æ®çš„ç¤ºä¾‹")


if __name__ == "__main__":
    main()